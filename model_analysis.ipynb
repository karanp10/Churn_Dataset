{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f6a4e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import category_encoders as ce\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import uniform, randint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7554c14",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>account_length</th>\n",
       "      <th>area_code</th>\n",
       "      <th>international_plan</th>\n",
       "      <th>voice_mail_plan</th>\n",
       "      <th>number_vmail_messages</th>\n",
       "      <th>total_day_minutes</th>\n",
       "      <th>total_day_calls</th>\n",
       "      <th>total_day_charge</th>\n",
       "      <th>total_eve_minutes</th>\n",
       "      <th>total_eve_calls</th>\n",
       "      <th>total_eve_charge</th>\n",
       "      <th>total_night_minutes</th>\n",
       "      <th>total_night_calls</th>\n",
       "      <th>total_night_charge</th>\n",
       "      <th>total_intl_minutes</th>\n",
       "      <th>total_intl_calls</th>\n",
       "      <th>total_intl_charge</th>\n",
       "      <th>number_customer_service_calls</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OH</td>\n",
       "      <td>107</td>\n",
       "      <td>area_code_415</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>26</td>\n",
       "      <td>161.6</td>\n",
       "      <td>123</td>\n",
       "      <td>27.47</td>\n",
       "      <td>195.5</td>\n",
       "      <td>103</td>\n",
       "      <td>16.62</td>\n",
       "      <td>254.4</td>\n",
       "      <td>103</td>\n",
       "      <td>11.45</td>\n",
       "      <td>13.7</td>\n",
       "      <td>3</td>\n",
       "      <td>3.70</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NJ</td>\n",
       "      <td>137</td>\n",
       "      <td>area_code_415</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>243.4</td>\n",
       "      <td>114</td>\n",
       "      <td>41.38</td>\n",
       "      <td>121.2</td>\n",
       "      <td>110</td>\n",
       "      <td>10.30</td>\n",
       "      <td>162.6</td>\n",
       "      <td>104</td>\n",
       "      <td>7.32</td>\n",
       "      <td>12.2</td>\n",
       "      <td>5</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OH</td>\n",
       "      <td>84</td>\n",
       "      <td>area_code_408</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>299.4</td>\n",
       "      <td>71</td>\n",
       "      <td>50.90</td>\n",
       "      <td>61.9</td>\n",
       "      <td>88</td>\n",
       "      <td>5.26</td>\n",
       "      <td>196.9</td>\n",
       "      <td>89</td>\n",
       "      <td>8.86</td>\n",
       "      <td>6.6</td>\n",
       "      <td>7</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OK</td>\n",
       "      <td>75</td>\n",
       "      <td>area_code_415</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>166.7</td>\n",
       "      <td>113</td>\n",
       "      <td>28.34</td>\n",
       "      <td>148.3</td>\n",
       "      <td>122</td>\n",
       "      <td>12.61</td>\n",
       "      <td>186.9</td>\n",
       "      <td>121</td>\n",
       "      <td>8.41</td>\n",
       "      <td>10.1</td>\n",
       "      <td>3</td>\n",
       "      <td>2.73</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MA</td>\n",
       "      <td>121</td>\n",
       "      <td>area_code_510</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>24</td>\n",
       "      <td>218.2</td>\n",
       "      <td>88</td>\n",
       "      <td>37.09</td>\n",
       "      <td>348.5</td>\n",
       "      <td>108</td>\n",
       "      <td>29.62</td>\n",
       "      <td>212.6</td>\n",
       "      <td>118</td>\n",
       "      <td>9.57</td>\n",
       "      <td>7.5</td>\n",
       "      <td>7</td>\n",
       "      <td>2.03</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  state  account_length      area_code international_plan voice_mail_plan  \\\n",
       "0    OH             107  area_code_415                 no             yes   \n",
       "1    NJ             137  area_code_415                 no              no   \n",
       "2    OH              84  area_code_408                yes              no   \n",
       "3    OK              75  area_code_415                yes              no   \n",
       "4    MA             121  area_code_510                 no             yes   \n",
       "\n",
       "   number_vmail_messages  total_day_minutes  total_day_calls  \\\n",
       "0                     26              161.6              123   \n",
       "1                      0              243.4              114   \n",
       "2                      0              299.4               71   \n",
       "3                      0              166.7              113   \n",
       "4                     24              218.2               88   \n",
       "\n",
       "   total_day_charge  total_eve_minutes  total_eve_calls  total_eve_charge  \\\n",
       "0             27.47              195.5              103             16.62   \n",
       "1             41.38              121.2              110             10.30   \n",
       "2             50.90               61.9               88              5.26   \n",
       "3             28.34              148.3              122             12.61   \n",
       "4             37.09              348.5              108             29.62   \n",
       "\n",
       "   total_night_minutes  total_night_calls  total_night_charge  \\\n",
       "0                254.4                103               11.45   \n",
       "1                162.6                104                7.32   \n",
       "2                196.9                 89                8.86   \n",
       "3                186.9                121                8.41   \n",
       "4                212.6                118                9.57   \n",
       "\n",
       "   total_intl_minutes  total_intl_calls  total_intl_charge  \\\n",
       "0                13.7                 3               3.70   \n",
       "1                12.2                 5               3.29   \n",
       "2                 6.6                 7               1.78   \n",
       "3                10.1                 3               2.73   \n",
       "4                 7.5                 7               2.03   \n",
       "\n",
       "   number_customer_service_calls churn  \n",
       "0                              1    no  \n",
       "1                              0    no  \n",
       "2                              2    no  \n",
       "3                              3    no  \n",
       "4                              3    no  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fb7461",
   "metadata": {},
   "source": [
    "This dataframe is a subset of a Telco churn dataset which records information about customers who have stayed or lef the company. The last column, \"churn\", indicates whether the customer has left the company or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a9f4845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset():\n",
    "    df_train = pd.read_csv('train.csv')\n",
    "\n",
    "    hash_encoder = ce.HashingEncoder(cols=['state'])\n",
    "    df_train = hash_encoder.fit_transform(df_train)\n",
    "    \n",
    "    df_train.international_plan.replace(['no', 'yes'], [0,1], inplace=True)\n",
    "    df_train.voice_mail_plan.replace(['no', 'yes'], [0,1], inplace=True)\n",
    "    df_train.churn.replace(['no', 'yes'], [0,1], inplace=True)\n",
    "\n",
    "    onehot_area = OneHotEncoder()\n",
    "    onehot_area.fit(df_train[['area_code']])\n",
    "    encoded_values = onehot_area.transform(df_train[['area_code']])\n",
    "    df_train[onehot_area.categories_[0]] = encoded_values.toarray()\n",
    "    df_train = df_train.drop('area_code', axis=1)\n",
    "\n",
    "    features = df_train.drop('churn', axis=1).values\n",
    "    target = df_train.churn.values\n",
    "\n",
    "    return features, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cb7baa",
   "metadata": {},
   "source": [
    "hash_encoder = ce.HashingEncoder(cols=['state'])\n",
    "df_train = hash_encoder.fit_transform(df_train)\n",
    "\n",
    "This line creates an instance of the HashingEncoder class from the category_encoders library. It passes in the argument (cols=['state'], indicating that the state column should be encoded using this method. The second line fits the encoder to the data and transforms it in one step. The HashingEncoder maps categorical values into numerical values. For example, states such as 'NJ', 'NYC' will be mapped into unique numerical categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c72cc6",
   "metadata": {},
   "source": [
    "df_train.international_plan.replace(['no', 'yes'], [0,1], inplace=True)\n",
    "df_train.voice_mail_plan.replace(['no', 'yes'], [0,1], inplace=True)\n",
    "df_train.churn.replace(['no', 'yes'], [0,1], inplace=True)\n",
    "\n",
    "These 3 lines of code performs a value replacement operation on international_plan, voice_mail_plan, and churn columns in the dataframe. In this case, the values 'no' and 'yes' in the columns are being replaced with the values 0 and 1, respectively. In effect, these lines of code are transforming the columns from a categorical variable with string values to a numerical variable with binary values (0 for 'no' and 1 for 'yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d370a27d",
   "metadata": {},
   "source": [
    "onehot_area = OneHotEncoder()\n",
    "onehot_area.fit(df_train[['area_code']])\n",
    "encoded_values = onehot_area.transform(df_train[['area_code']])\n",
    "df_train[onehot_area.categories_[0]] = encoded_values.toarray()\n",
    "df_train = df_train.drop('area_code', axis=1)\n",
    "\n",
    "The next part of the data cleaning process was to encode the area code values as their numerical value wouldn't provide any use to the models. Each row of the original categorical variable will be transformed into a row of n binary variables, with only one variable having the value of 1 (indicating the presence of the corresponding category) and the rest having the value of 0. By converting categorical variables into binary variables, one-hot encoding allows categorical data to be used as input to these algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dfca5850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_split(features, target):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(features, target, test_size=0.3)\n",
    "    #sm = SMOTE(sampling_strategy = 1, random_state=1)\n",
    "\n",
    "    #X_train, y_train = sm.fit_resample(X_train, y_train.ravel())\n",
    "\n",
    "    #scaler = MinMaxScaler()\n",
    "    #X_train = scaler.fit_transform(X_train)\n",
    "    #X_val = scaler.transform(X_val)\n",
    "\n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b5cd45",
   "metadata": {},
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(features, target, test_size=0.3)\n",
    "\n",
    "This function takes in two inputs, features and target and then the train_test_split is used to split the features and the target into training and validation sets. The input data is split into training (70%) and validation (30%) sets by specifying test_size=0.3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430caa13",
   "metadata": {},
   "source": [
    "sm = SMOTE(sampling_strategy = 1, random_state=1)\n",
    "\n",
    "X_train, y_train = sm.fit_resample(X_train, y_train.ravel())\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "The commented out code contains the SMOTE function from the imblearn.over_sampling library which is used to create synthethic samples of the minority class. With the sampling_strategy argument set to 1, the number of synthetic samples to be generated for the minority class will be equal to the number of samples in the majority class. The MinMax Scaler is used to scale the features of the training and validation sets. The accuracy and the precision of the model was reduced however with these features present, hence the comments.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a2aa5851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(X_train, X_val, y_train, y_val):\n",
    "    classifier = RandomForestClassifier(n_estimators=80, min_samples_split=5, max_depth=29, random_state=1)\n",
    "    classifier = classifier.fit(X_train, y_train)\n",
    "    y_predictions = classifier.predict(X_val)\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_val, y_predictions)\n",
    "    accuracy = cross_val_score(classifier, X_train, y_train, cv=5, scoring='accuracy').mean()\n",
    "    precision = cross_val_score(classifier, X_train, y_train, cv=5, scoring='precision').mean()\n",
    "\n",
    "    #param_dist = {\n",
    "    #'n_estimators': np.arange(10, 100, 10),\n",
    "    #'max_depth': np.arange(5, 30),\n",
    "    #'min_samples_split': np.arange(2, 12)}\n",
    "\n",
    "    #random_search = RandomizedSearchCV(classifier, param_distributions=param_dist, n_iter=100, cv=5, scoring='accuracy')\n",
    "    #random_search.fit(X_train, y_train)\n",
    "\n",
    "    #best_params = random_search.best_params_\n",
    "    #print(best_params)\n",
    "\n",
    "    print('RF Accuracy Score: ',accuracy)\n",
    "    print('RF Precision Score: ',precision)\n",
    "    print('RF Confusion Matrix: ',conf_matrix)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bca20e",
   "metadata": {},
   "source": [
    "classifier = RandomForestClassifier(n_estimators=80, min_samples_split=5, max_depth=29, random_state=1) \n",
    "classifier = classifier.fit(X_train, y_train)\n",
    "y_predictions = classifier.predict(X_val)\n",
    "\n",
    "The use of a Random Forest Model as the initial algorithm for this classification test was chosen due to its versatility and efficacy in handling various factors. As a non-linear model, it remains suitable to handle datasets with complex relationships. Furthermore, it provides feature importance values, which can be useful in understanding which features are the most significant in predicting churn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4048dfa7",
   "metadata": {},
   "source": [
    "conf_matrix = confusion_matrix(y_val, y_predictions)\n",
    "accuracy = cross_val_score(classifier, X_train, y_train, cv=5, scoring='accuracy').mean()\n",
    "precision = cross_val_score(classifier, X_train, y_train, cv=5, scoring='precision').mean()\n",
    "\n",
    "TP (True Positives)    FP (False Positives)\n",
    "FN (False Negatives)    TN (True Negatives)\n",
    "\n",
    "[[1080  7][137  51]]\n",
    "\n",
    "Above, is an example of a confusion matrix. The array [1080  7][137  51]] represents the amount of each condition, in this case being 1080 True Positives, 7 False Positives, 137 False Negatives, and 51 False Negatives. Accuracy is the proportion of correctly classified samples over all samples. It is calculated as (TP + TN) / (TP + TN + FP + FN). Precision is the proportion of correctly classified positive samples over all samples classified as positive. It is calculated as TP / (TP + FP). The cross_val_score function obtains the mean accuracy and precision of the model using 5 folds. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bab2a28",
   "metadata": {},
   "source": [
    "classifier = RandomForestClassifier(n_estimators=80, min_samples_split=5, max_depth=29)\n",
    "\n",
    "param_dist = {\n",
    "'n_estimators': np.arange(10, 100, 10),\n",
    "'max_depth': np.arange(5, 30),\n",
    "'min_samples_split': np.arange(2, 12)}\n",
    "\n",
    "random_search = RandomizedSearchCV(classifier, param_distributions=param_dist, n_iter=100, cv=5,scoring='accuracy')\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = random_search.best_params_\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c859969c",
   "metadata": {},
   "source": [
    "The commented out code represents hyperparameter tuning using randomized search cross-validation (RandomizedSearchCV) on the RandomForestClassifier model. The param_dist defines the hyperparameters to be tuned and their respective search ranges. The random_search creates an instance of the RandomizedSearchCV the hyperparameters to be tuned and their search ranges defined in param_dist, the number of iterations, the number of folds to use in cross-validation, and the scoring metric to be used. This means that after using RandomizedSearchCV to find the best hyperparameters for the RandomForestClassifier, the values were then incorporated into the model for improved performance as shown in the first line. The n_estimators parameter defines the number of \"trees\" in the forest, more trees implies a better model performance, but a longer training time. The max_depth parameter determines the maximum depth of each tree in the performance, increasing it could lead to more complex trees but also overfitting. The min_samples_split sets the minimum number of samples required to split an internal node in the tree, however increasing it too much could also lead to overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c7d2044a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb(X_train, X_val, y_train, y_val):\n",
    "    classifier = XGBClassifier(max_depth=9, gamma=0.480, learning_rate=0.150, min_child_weight=1, n_estimators=259, subsample=0.5851, colsample_bytree=0.541,random_state=1)\n",
    "    classifier = classifier.fit(X_train, y_train)\n",
    "    y_predictions = classifier.predict(X_val)\n",
    "\n",
    "    #param_grid = {\n",
    "    #\"learning_rate\": uniform(0, 1),\n",
    "    #\"max_depth\": randint(1, 10),\n",
    "    #\"n_estimators\": randint(50, 500),\n",
    "    #\"min_child_weight\": randint(1, 10),\n",
    "    #\"subsample\": uniform(0.1, 1),\n",
    "    #\"gamma\": uniform(0, 1),\n",
    "    #\"colsample_bytree\": uniform(0.1, 1)}\n",
    "\n",
    "    #random_search = RandomizedSearchCV(classifier, param_grid, cv=5, n_iter=100, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "    #random_search.fit(X_train, y_train)\n",
    "\n",
    "    #print(random_search.best_params_)\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_val, y_predictions)\n",
    "    accuracy = cross_val_score(classifier, X_train, y_train, cv=5, scoring='accuracy').mean()\n",
    "    precision = cross_val_score(classifier, X_train, y_train, cv=5, scoring='precision').mean()\n",
    "\n",
    "    print('\\n')\n",
    "    print('XGB Accuracy Score: ',accuracy)\n",
    "    print('XGB Precision Score: ',precision)\n",
    "    print('XGB Confusion Matrix: ',conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f6f9cc",
   "metadata": {},
   "source": [
    "classifier = XGBClassifier(max_depth=9, gamma=0.480, learning_rate=0.150, min_child_weight=1, n_estimators=259, subsample=0.5851, colsample_bytree=0.541, random_state=1)\n",
    "classifier = classifier.fit(X_train, y_train)\n",
    "y_predictions = classifier.predict(X_val)\n",
    "\n",
    "The next model chosen was the XGBoost ML algorithim which is a highly flexible and customizable model allowing you to adjust hyperparameters for optimal performance. It is considered a an advanced implentation of the gradient boosting algorithim and has also shown to produce great results. The learning rate hyperparameter controls the step size at which the optimizer makes updates to the model weights. Smaller values will make the model converge slower, but will typically result in a better fit. Subsample specifies the fraction of the training data to use for each tree. Smaller values can result in better models, but at a risk of underfitting the data. Gamma specifies the minimum reduction in the loss function required to split further. Increaing this value can result in fewer splits which could then result in overfitting. The same random_search was applied to this model to generate the optimal hyperparameters for the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d62bca35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_neighbors(X_train, X_val, y_train, y_val):\n",
    "    classifier = KNeighborsClassifier(metric='manhattan', n_neighbors=14, weights='distance')\n",
    "    classifier = classifier.fit(X_train, y_train)\n",
    "    y_predictions = classifier.predict(X_val)\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_val, y_predictions)\n",
    "    accuracy = cross_val_score(classifier, X_train, y_train, cv=5, scoring='accuracy').mean()\n",
    "    precision = cross_val_score(classifier, X_train, y_train, cv=5, scoring='precision').mean()\n",
    "\n",
    "    #param_grid = {'n_neighbors': np.arange(1, 50),\n",
    "    #          'weights': ['uniform', 'distance'],\n",
    "    #          'metric': ['euclidean', 'manhattan']}\n",
    "    \n",
    "    #rand_search = RandomizedSearchCV(classifier, param_distributions=param_grid, n_iter=100, cv=5, n_jobs=-1)\n",
    "    #rand_search.fit(X_train, y_train)\n",
    "    #best_params = rand_search.best_params_\n",
    "    #best_estimator = rand_search.best_estimator_\n",
    "    #best_score = rand_search.best_score_\n",
    "\n",
    "    #print(\"Best parameters: \", best_params)\n",
    "    #print(\"Best score: \", best_score)\n",
    "    \n",
    "    print('\\n')\n",
    "    print('KNN Accuracy Score: ',accuracy)\n",
    "    print('KNN Precision Score: ',precision)\n",
    "    print('KNN Confusion Matrix: ',conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faab6bf5",
   "metadata": {},
   "source": [
    "classifier = KNeighborsClassifier(metric='manhattan', n_neighbors=14, weights='distance')\n",
    "classifier = classifier.fit(X_train, y_train)\n",
    "y_predictions = classifier.predict(X_val)\n",
    "\n",
    "KNeighbors was the next model chosen as it was a non-parametric model, meaning it doesn't make any assumptions about the distribution of data which could be useful when dealing with datasets that have complex relationships. N_neighbors represents the number of nearest neighbors to consider while making predictions and large values of this can result in overfitting the model. The weight function is used to define the distance between the nearest neighbors. 'Distance' assigns weights proportional to the inverse of the distance to the nearest neighbors. Metric is used to compute the distances between the instances. The same random_search was again applied to this model to generate optimal hyperparameters for the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d852ead3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Accuracy Score:  0.9556302521008403\n",
      "RF Precision Score:  0.9624919655082028\n",
      "RF Confusion Matrix:  [[1082   13]\n",
      " [  59  121]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "XGB Accuracy Score:  0.9542857142857143\n",
      "XGB Precision Score:  0.9203285653378117\n",
      "XGB Confusion Matrix:  [[1079   16]\n",
      " [  53  127]]\n",
      "\n",
      "\n",
      "KNN Accuracy Score:  0.891764705882353\n",
      "KNN Precision Score:  0.8463120052876787\n",
      "KNN Confusion Matrix:  [[1089    6]\n",
      " [ 129   51]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":   \n",
    "    features, target = clean_dataset()\n",
    "    X_train, X_val, y_train, y_val = load_train_split(features, target)\n",
    "    random_forest(X_train, X_val, y_train, y_val)\n",
    "    xgb(X_train, X_val, y_train, y_val)\n",
    "    k_neighbors(X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6035fb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
